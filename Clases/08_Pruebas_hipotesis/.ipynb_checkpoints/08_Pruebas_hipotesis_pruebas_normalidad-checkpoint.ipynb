{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26d99c13-4cc8-49a3-9fec-a5c710c5084f",
   "metadata": {},
   "source": [
    "<font color='IndianRed'>\n",
    "<font size=9> \n",
    "\n",
    "**Curso Inferencia Estadística**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdbf546-5c5d-4eda-a48a-6e1b13fff62d",
   "metadata": {},
   "source": [
    "<font color = 'DodgerBlue'>\n",
    "<font size = 5>\n",
    "    \n",
    "**Pruebas de normalidad**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2be8c9a-dc56-4c48-a234-6460ba8ed1cc",
   "metadata": {},
   "source": [
    "<font color = 'Black'>\n",
    "<font size = 3>\n",
    "\n",
    "Frecuentemente nos encontramos frente al problema de reconocer si una muestra proviene de una **población gaussiana**.\n",
    "\n",
    "Aunque ya hemos visto **ajuste de distribuciones**, para el caso de **gaussianas** tenemos más métodos para establecer, con cierta probabilidad, si una muestra proviene realmente de una **distribución normal**. Estos métodos están basados en **pruebas de hipótesis**, donde la **hipótesis nula** tiene la forma **\"la población es gaussiana\"**, en tanto que la **alternativa** es **\"la población no es gaussiana\"**.\n",
    "\n",
    "De esta manera, a cada una de las pruebas se le asigna un **estadístico de contraste** con el cual se calcula un $p$-valor, de modo que podemos escribirlas así:\n",
    "\n",
    "$$\\left\\{\\begin{array}{l}H_0:\\mbox{ la población es gaussiana}\\\\H_1:\\mbox{ la población no es gaussiana}\\end{array}\\right.$$\n",
    "\n",
    "> ⚠️ **Nota importante: El p-valor es una **probabilidad** y siempre toma valores $i \\in [0,1]$. Si es muy cercano a 1, hay probabilidad de que si haya gaussianidad**.\n",
    "\n",
    "con **regla de decisión**:\n",
    "\n",
    "$p$-valor|Decisión|Significado\n",
    ":--|:--|:--\n",
    "Pequeño|Rechazar $H_0$|Hay buena probabilidad de que **NO es gaussiana**\n",
    "Grande|Rechazar $H_1$|Hay buena probabilidad de que **SÍ es gaussiana**\n",
    "\n",
    "En este capítulo estudiaremos cuatro contrastes, también conocidad como **pruebas de bondad y ajuste** (pruebas paramétricas para verificar si hay gaussianidad) que nos ayudan para este fin:\n",
    "\n",
    "1. **Prueba de Shapiro-Wilk**: Una de las pruebas más populares y potentes para detectar desviaciones de la normalidad, especialmente en muestras pequeñas.\n",
    "2. **Prueba de Kolmogorov-Smirnov**: Una prueba basada en la comparación entre la distribución empírica y la distribución normal teórica. \n",
    "3. **Prueba de Anderson-Darling**: Una extensión del contraste de Kolmogorov-Smirnov que da más peso a las colas de la distribución.\n",
    "4. **Prueba de Jarque-Bera**: Evalúa la normalidad basándose en los momentos estadísticos de asimetría y curtosis.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95280812-d2da-4a83-9783-36b6234f6a68",
   "metadata": {},
   "source": [
    "<font color = 'DodgerBlue'>\n",
    "<font size = 5>\n",
    "    \n",
    "**Contexto e importancia de las pruebas de normalidad**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26917bcd-4b99-45e1-9819-f7b77b55ab03",
   "metadata": {},
   "source": [
    "<font color = 'Black'>\n",
    "<font size = 3>\n",
    "\n",
    "En estadística, muchas pruebas y modelos (como **ANOVA**, **regresión lineal**, **t de Student**, entre otros) requieren que los datos sigan una **distribución normal**. Por ello, las **pruebas de normalidad** son un paso esencial para garantizar la validez de los resultados.\n",
    "\n",
    "Es importante considerar también que:\n",
    "\n",
    "- **Tamaños de muestra pequeños**: Las pruebas pueden tener poca potencia, y un histograma o gráfico Q-Q puede complementar la evaluación.\n",
    "- **Tamaños de muestra grandes**: Las pruebas tienden a detectar pequeñas desviaciones que pueden no ser prácticamente significativas.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46610f32-c1a6-47e7-a23a-bcfd207129d8",
   "metadata": {},
   "source": [
    "<font color = 'DodgerBlue'>\n",
    "<font size = 5>\n",
    "    \n",
    "**Pruebas en Python**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7869ec4-d15e-41d6-9d0b-6d71fc62a438",
   "metadata": {},
   "source": [
    "<font color = 'Black'>\n",
    "<font size = 3>\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import scipy.stats as stats, lognorm\n",
    "import random\n",
    "\n",
    "\n",
    "#generada con N(mu=3.5,sigma=2)\n",
    "small_gauss = [2.9267007, 2.5763093, 4.9931801, 0.6564296, 1.4377333, 7.6412183, 2.9204735] \n",
    "\n",
    "#generada con t(3)\n",
    "big_t = [-0.577103929579228, -0.0669625949987604, 0.123572935953355, -0.524985500797433, -1.23249669279686, 0.509597230395874, -0.729559305649031, -0.41684441016622, 1.28155478163868, 0.924508782035897, 0.827405247774813, 1.59785194962189, -1.47879497630707, -1.26201626124022, -0.0593983026205043, -0.178873361732746, 0.801185847793428, 0.333473064862654, 1.25186288055626, 2.35949695172828, -0.633493106081742, -1.05713142223298, 0.0212461334293823, 0.466063027431909, 0.0762121526958427, -0.843837287109611, -0.104022595760381, 5.78550093074697, 0.709799846598426, -0.0897824055310009, -0.999402655342385, 0.337761665033848, -0.0306307006025367, 1.47728344947859, -0.176164802725808, 0.690341335235668, -0.292183630229324, -0.844902899428558, -3.49551302890857, 1.43006662844371, 1.24850000914668, -0.180820066444685, -0.573485189819109, 0.349757398842014, -2.09754115696913, -0.352572352149588, -0.509125036161415, 0.712742491824159, 0.519051722042105, -3.00737218678664]\n",
    "\n",
    "#generada con N(mu=5,sigma=1)\n",
    "random.seed(2024)\n",
    "big_gauss = stats.norm(scale=1, loc=5).rvs(1000)\n",
    "\n",
    "# Tomado de https://webspace.ship.edu/pgmarr/Geo441/Lectures/Lec%205%20-%20Normality%20Testing.pdf\n",
    "densidades_mexico = {\n",
    "    'Region': [\n",
    "        'Ajuno', 'Angahuan', 'Arantepacua', 'Aranza', 'Charapan', 'Cheran',\n",
    "        'Cocucho', 'Comachuen', 'Corupo', 'Ihuatzio', 'Janitzio', 'Jaracuaro',\n",
    "        'Nahuatzen', 'Nurio', 'Paracho', 'Patzcuaro', 'Pichataro',\n",
    "        'Pomacuaran', 'Quinceo', 'Quiroga', 'San Felipe', 'San Lorenzo',\n",
    "        'Sevina', 'Tingambato', 'Turicuaro', 'Tzintzuntzan', 'Urapicho'\n",
    "    ],\n",
    "    'Population_Density': [\n",
    "        5.11, 5.15, 5.00, 4.13, 5.10, 5.22, 5.04, 5.25, 4.53, 5.74, 6.63, 5.73,\n",
    "        4.77, 6.06, 4.82, 4.98, 5.36, 4.96, 5.94, 5.01, 4.10, 4.69, 4.97, 5.01,\n",
    "        6.19, 4.67, 6.30\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Generada con lognorm con media e y desviación 0.5\n",
    "np.random.seed(1)\n",
    "mi_lognorm = lognorm.rvs(s=.5, scale=math.exp(1), size=1000)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed2878e-a47c-437c-8805-d024269e8d94",
   "metadata": {},
   "source": [
    "<font color = 'DodgerBlue'>\n",
    "<font size = 5>\n",
    "\n",
    "---\n",
    "    \n",
    "**1. Prueba de Shapiro-Wilk**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6687063-3058-4e85-98f1-574167274db4",
   "metadata": {},
   "source": [
    "<font color = 'Black'>\n",
    "<font size = 3>\n",
    "\n",
    "La prueba de **Shapiro-Wilk** evalúa si una muestra sigue una **distribución normal** al calcular un estadístico $W$ basado en el orden de los datos. Funciona bien para tamaños tan bajos como muestras de tamaño 3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20cdcaf-cbb7-4705-ba84-14bb10ad6353",
   "metadata": {},
   "source": [
    "<font color = 'DodgerBlue'>\n",
    "<font size = 5>\n",
    "    \n",
    "**Implementación en Python**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a25a5cc-caec-47b8-b921-b879024c9f12",
   "metadata": {},
   "source": [
    "<font color = 'Black'>\n",
    "<font size = 3>\n",
    "\n",
    "Usaremos la función `shapiro` de la biblioteca `scipy.stats`.\n",
    "\n",
    "```python\n",
    "from scipy.stats import shapiro\n",
    "\n",
    "# Prueba de Shapiro-Wilk\n",
    "stat, p_value = shapiro(data)\n",
    "\n",
    "print(f\"Estadístico W: {stat:.4f}\")\n",
    "print(f\"p-valor: {p_value:.4f}\")\n",
    "\n",
    "# Regla de decisión\n",
    "if p_value < 0.05:\n",
    "    print(\"Rechazamos H0: La muestra no sigue una distribución normal\")\n",
    "else:\n",
    "    print(\"No podemos rechazar H0: La muestra sigue una distribución normal\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8071b1a5-5651-421c-aab3-9e198d9ae068",
   "metadata": {},
   "source": [
    "<font color = 'DodgerBlue'>\n",
    "<font size = 5>\n",
    "\n",
    "---\n",
    "    \n",
    "**2. Prueba de Kolmogorov-Smirnov**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4126a481-81b6-459b-97a1-a3d504dfe201",
   "metadata": {},
   "source": [
    "<font color = 'Black'>\n",
    "<font size = 3>\n",
    "\n",
    "Esta prueba compara la distribución empírica de los datos con la distribución normal teórica.\n",
    "\n",
    "La idea sobre esta prueba es la siguiente, usando la **distancia al infinito** o **métrica del supremo**. Imaginemos que tenemos una curva (color azul) y digamos que tenemos una banda (color verde) que es idéntica a la curva azul pero desplazada, por lo que cubre a la curva azul.\n",
    "\n",
    "Ahora, si tenemos otra curva (color rojo), podemos decir que la curva roja se parece (en el sentido de topología) a la curva azul si queda por dentro de la fraja verde. Si el diámetro de la franja verde es muy pequeña, se podría decir que si queda la curva dentro de ella se parece.\n",
    "\n",
    "La idea geométrica se muestra en la siguiente imagen:\n",
    "\n",
    "<img src = \"Img/dist_inf.jpg\">\n",
    "\n",
    "Supongamos que tienen tus datos, lo que hace **Kolmogorov-Smirnov** es calcular la **media** y la **desviación** de los datos, luego hace una curva gaussiana con esa **media** y esa **desviación**. Si tus datos son realmente **gaussianos**, al hacer el histograma deberían quedar muy ajustados a esa curva. Es decir, que los rectángulos de ese histograma queden por dentro de esa franja. Así como se muestra en la siguiente imagen:\n",
    "\n",
    "<img src = \"Img/dist_inf2.jpg\">\n",
    "\n",
    "La **métrica del supremo**: \n",
    "\n",
    "$$d_\\infty(x, y) = \\max_{1 \\leq i \\leq n} |x_i - y_i|$$\n",
    "\n",
    "esta métrica es la que te define **convergencia de uniforme de funciones**, viene dada por la topología de la **norma del supremo**: \n",
    "\n",
    "$$\\|f\\|_\\infty = \\sup_{x \\in A} |f(x)|$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe674af-cdc1-4095-bed5-1cc28408316d",
   "metadata": {},
   "source": [
    "<font color = 'DodgerBlue'>\n",
    "<font size = 5>\n",
    "    \n",
    "**Implementación en Python**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9dd10a7-c2d4-49b6-ad5a-fa8543067f10",
   "metadata": {},
   "source": [
    "<font color = 'Black'>\n",
    "<font size = 3>\n",
    "\n",
    "Usaremos la función ``kstest`` de ``scipy.stats``.\n",
    "\n",
    "```python\n",
    "from scipy.stats import kstest, norm\n",
    "\n",
    "# Prueba de Kolmogorov-Smirnov\n",
    "from statistics import mean, stdev\n",
    "stat, p_value = kstest(data, 'norm', args=(mean(data), stdev(data)))\n",
    "\n",
    "print(f\"Estadístico KS: {stat:.4f}\")\n",
    "print(f\"p-valor: {p_value:.4f}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf208938-0a83-4bf8-86ec-ebfd382e1d42",
   "metadata": {},
   "source": [
    "<font color = 'DodgerBlue'>\n",
    "<font size = 5>\n",
    "\n",
    "---\n",
    "    \n",
    "**3. Prueba de Anderson-Darling**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d524b812-9356-43ee-b4c1-07af8cbf2efc",
   "metadata": {},
   "source": [
    "<font color = 'Black'>\n",
    "<font size = 3>\n",
    "\n",
    "La prueba de Anderson-Darling ajusta el estadístico de Kolmogorov-Smirnov para dar más peso a las colas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420c4389-7e66-4778-bdfe-1dc07339df83",
   "metadata": {},
   "source": [
    "<font color = 'DodgerBlue'>\n",
    "<font size = 5>\n",
    "    \n",
    "**Implementación en Python**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527c1a5e-2e4f-421a-9a44-2c65ecda6c0d",
   "metadata": {},
   "source": [
    "<font color = 'Black'>\n",
    "<font size = 3>\n",
    "\n",
    "Usaremos la función ``anderson`` de ``scipy.stats``.\n",
    "\n",
    "```python\n",
    "from scipy.stats import anderson\n",
    "\n",
    "# Prueba de Anderson-Darling\n",
    "result = anderson(data, dist='norm')\n",
    "\n",
    "print(f\"Estadístico A: {result.statistic:.4f}\")\n",
    "for i, sig in enumerate(result.significance_level):\n",
    "    print(f\"Nivel de significancia {sig}%: Valor crítico = {result.critical_values[i]:.4f}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe8f071-268e-4c99-a8af-923ee24e0081",
   "metadata": {},
   "source": [
    "<font color = 'DodgerBlue'>\n",
    "<font size = 5>\n",
    "\n",
    "---\n",
    "    \n",
    "**4. Prueba de Jarque-Bera**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4094fb5-caca-4d20-9457-87f3a7b6709e",
   "metadata": {},
   "source": [
    "<font color = 'Black'>\n",
    "<font size = 3>\n",
    "\n",
    "Esta prueba evalúa la normalidad mediante los coeficientes de **asimetría** y **curtosis**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68be3070-0ed8-4d4e-9133-e1ad62ad45a4",
   "metadata": {},
   "source": [
    "<font color = 'Black'>\n",
    "<font size = 3>\n",
    "\n",
    "Los **momentos** de una **distribución** se definen de la siguiente manera:\n",
    "\n",
    "$\\mu^{\\prime}_{n} = E[X^{n}] =  \\displaystyle \\int_{-\\infty}^{\\infty} x^{n} \\,dF(x)$\n",
    "\n",
    "donde $X$ es la **variable aleatoria** que tiene esta **distribución** acumulada $F$, y $E$ es la **esperanza** o **media**.\n",
    "\n",
    "La **media** de una **distribución** es la **esperanza** $E[X]$, está muy relacionada con la **media muestral** $\\overline{X}$.\n",
    "\n",
    "La **varianza** $Var(X) = E[X^{2}] - E[X]^{2}$, como $E[X^{2}]$ es el segundo **momento**. Entonces, la **varianza** está muy relacionada con el segundo **momento**.\n",
    "\n",
    "La **esperanza** de $E[X^{2}]$ viene siendo el tercer **momento**, está muy relacionad con otro parámetro denominado **asimetría** (es una idea de qué tan iguales son las colas), el siguiente **momento** es la **curtosis**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6e87ed-be46-49cd-ac9d-db907b7897a1",
   "metadata": {},
   "source": [
    "<font color = 'DodgerBlue'>\n",
    "<font size = 5>\n",
    "    \n",
    "**Implementación en Python**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c480e5e4-61e6-4b19-87e9-ef55febc869e",
   "metadata": {},
   "source": [
    "<font color = 'Black'>\n",
    "<font size = 3>\n",
    "Usaremos la función ``jarque_bera`` de ``scipy.stats``.\n",
    "\n",
    "```python\n",
    "from scipy.stats import jarque_bera\n",
    "\n",
    "# Prueba de Jarque-Bera\n",
    "stat, p_value = jarque_bera(data)\n",
    "\n",
    "print(f\"Estadístico JB: {stat:.4f}\")\n",
    "print(f\"p-valor: {p_value:.4f}\")\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0c7140-942a-430e-92a6-870036b1fd48",
   "metadata": {},
   "source": [
    "<font color = 'Brown'>\n",
    "<font size = 4>\n",
    "    \n",
    "**Notas**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edbc6a4b-a559-4f01-a4f5-97966037579b",
   "metadata": {},
   "source": [
    "<font color = 'DarkRed'>\n",
    "<font size = 3>\n",
    "\n",
    "- El p-valor es una **probabilidad** y siempre toma valores $i \\in [0,1]$.\n",
    "- Cuando tienes presencia de **datos atípicos**, estos pueden afectar las colas de la **distribución**, por lo que es necesario hacer una prueba de normalidad. Podría parecer que al centro si se vea como una gaussiana pero en las colas ya no hay simetría.\n",
    "- Si tienes una **muestra** pequeña vas a tener problemas, lo que te puede llegar a salvar es ver que pequeña **muestra** es **gaussiana**, porque siendo **gaussiana** ya no te interesa el tamaño, ya que la suma de **gaussianas** es también **gaussiana**, el **promedio** de **gaussianas** es **gaussiana**.\n",
    "- Cuando no tienes **gaussianidad** el **Teorema del Límite Central** te garantiza que los **promedios** si son **gaussianos**.\n",
    "- La peor situación es **muestra pequeña** sin **gaussianidad**.\n",
    "- En **regresiones lineales** nos interesa que los **errores cometidos** se comporten como **distribuciones gaussianas**, si no se comportan como **distribuciones gaussianas** pierdes mucha información en las **regresiones lineales**.\n",
    "> ⚠️ **Nota importante: Si los **errores cometidos** no forman una **gaussiana**, ya valió, ya no puedes continuar**.\n",
    "- Para **muestras** pequeñas, una última esperanza es que la **prueba de Shapiro-Wilk** salga positiva para **gaussianidad**. Si sale que no es gaussiana estás en problemas, porque estarías en el peor de los casos."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
